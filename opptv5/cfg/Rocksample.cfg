# General-purpose settings.
verbose = false
logPath = log
overwriteExistingLogFiles = true
logFilePostfix =
saveParticles = true

[plugins]
heuristicPlugin = libRocksampleHeuristicPlugin.so

planningRewardPlugin = librocksampleRewardPlugin.so
executionRewardPlugin = librocksampleRewardPlugin.so

planningTerminalPlugin = librocksampleTerminalPlugin.so
executionTerminalPlugin = librocksampleTerminalPlugin.so

planningTransitionPlugin = librocksampleTransitionPlugin.so
executionTransitionPlugin = librocksampleTransitionPlugin.so

planningObservationPlugin = librocksampleObservationPlugin.so
executionObservationPlugin = librocksampleObservationPlugin.so

executionInitialBeliefPlugin = librocksampleInitialBeliefPlugin.so
planningInitialBeliefPlugin = librocksampleInitialBeliefPlugin.so

[observationPluginOptions]
observationError = 20.0

[rewardPluginOptions]
stepPenalty = 1
illegalMovePenalty = 500
exitReward = 1000

[heuristicPluginOptions]
planningRange = 0.04

[initialBeliefOptions]
initialState = [1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

[problem]
# Number of simulation runs
nRuns = 3

# Maximum number of steps to reach the goal
nSteps = 150

# The planning environment SDF
planningEnvironmentPath = RocksampleEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = RocksampleEnvironment.sdf

# The robot SDF model
robotName = RocksampleRobot

enableGazeboStateLogging = true

# The discount factor of the reward model
discountFactor = 0.95

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################

[state]
linkPositionsX = [RocksampleRobot::RocksampleLink]
linkPositionsY = [RocksampleRobot::RocksampleLink]

linkPositionXLimits = [[-1, 9]]
linkPositionYLimits = [[-1, 9]]

# Rock states
additionalDimensions = 8

# Rock states can either be "0" for bad rocks and "1" for good rocks
additionalDimensionLimits = [[0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1]]

[action]
additionalDimensions = 1

# Encodes 13 discrete actions
additionalDimensionLimits = [[1, 13]]

[observation]
additionalDimensions = 1
additionalDimensionLimits = [[-1, 1]]

[changes]
hasChanges = false
changesPath = changes_1.txt
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 10000

# The maximum L2-distance between observations for them to be considered similar
maxObservationDistance = 0.0005

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

maximumDepth = 900

searchStrategy = ucb(5.0)

estimator = mean()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
numInputStepsActions = 13

observationType = discrete
numInputStepsObservation = 3

[simulation]
interactive = false
particlePlotLimit = 0
