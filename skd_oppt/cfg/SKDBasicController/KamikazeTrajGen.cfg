# General-purpose settings.
verbose = true
logPath = /home/jimy/jimy_oppt_ws/ncap_oppt_devel/NCAP_OPPT/experimental_data/03-05/BasicSafety/03-05-16-28-49 
overwriteExistingLogFiles = true
logFilePostfix = 03-05-16-28-49_basic_safety_data_g_18_k_8_m_1.125 
saveParticles = false


############################## PLUGIN SHARED LIBRARIES TO LOAD #############################

[plugins]

# Checked
executionInitialBeliefPlugin = libKamikazeTrajGenInitialBeliefPlugin.so
planningInitialBeliefPlugin = libKamikazeTrajGenInitialBeliefPlugin.so


# Checked
planningObservationPlugin = libSKDGenObservationPlugin.so
executionObservationPlugin = libSKDGenObservationPlugin.so

# Checked
planningRewardPlugin = libKamikazeTrajGenRewardPlugin.so
executionRewardPlugin = libKamikazeTrajGenRewardPlugin.so


# Checked
planningTransitionPlugin = libKamikazeTrajGenTransitionPlugin.so
executionTransitionPlugin = libKamikazeTrajGenTransitionPlugin.so

# Needs fixing
heuristicPlugin = libKamikazeTrajGenHeuristicPlugin.so


# Checked
planningTerminalPlugin = libKamikazeTrajGenTerminalPlugin.so
executionTerminalPlugin = libKamikazeTrajGenTerminalPlugin.so


############################ PLUGIN SPECIFIC OPTIONS ##################################
[initialBeliefOptions]
lowerBound = [120, 3, 100, -1.7, 8.33, 3]
upperBound = [120, 3, 100, -1.7, 8.33, 3]


[transitionPluginOptions]
# fix time value between action steps (seconds)
fixedStepTime = 0.3
processError = 0

[observationPluginOptions]
#Observaion Error
carObsError = 2


[rewardPluginOptions]
goalReward = 5000
stepPenalty = 5
terminalPenalty = 5000


[terminalPluginOptions]
# Set avoided distance to 5 metres for now
avoidedDistance = 3



############################## THIS SECTION IS FOR  SPECIFIC VALUES TO THE SKD_PROBLEM #############################
[generalOptions]
# Speed limit in m/s
speedLimit = 5.55
carFOV = 1.5

# Braking deceleration in m/(s)^2 
brakingDeceleration = -3.5
initialVelocity = 8.33
controllerMultiplier = 1.125 

# Naming of models in the environment
carLinkName = CarLink
pedLinkName = PedestrianLink

# Car dimension
carDimensions = [4.68 1.88 1.3]
pedDimensions = [0.34, 1.86]

#Safety goal area
safetyGoalArea = [8.74, -4.25]
goalMargins = [0.5, 0.5]

# Discretization of the intention data
intentionDiscretization = [15, 15]
intentionDiscretizationLower = [-8.75, -6.72]
intentionDiscretizationUpper = [20, 5.85]



# Safe trajectory file path
safeTrajFilePath = /home/jimy/jimy_oppt_ws/ncap_oppt_devel/NCAP_OPPT/safe_trajs_json/12-02-21/goal_18_safe_trajs.json 
safeTrajIndex = 8 



################################### EXPERIMENT SPECIFIC OPTIONS #############################
[problem]
# Number of simulation runs
nRuns = 2 

# Maximum number of steps to reach the goal
nSteps = 25

# The planning environment SDF
planningEnvironmentPath = SKDGenTestingEnvironmnent.sdf

# The execution environment SDF
executionEnvironmentPath = SKDGenTestingEnvironmnent.sdf

# The robot SDF model
robotName = Pedestrian

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.98

# Using state- action- and observation spaces that are normalized to [0, 1]
normalizedSpaces = false

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 300


[state]
# State space description
# [PED_X, PED_Y, CAR_X, CAR_Y, CAR_VEL, INTENTION]
# Additional dimension represents the intention of the car [0:CRUISING, 1:EMERGENCY_STOP]
additionalDimensions = 6



[action]
# Action space is only x,y displacement for the pedestrian
additionalDimensions = 2
additionalDimensionLimits = [[0, 2.5], [0, 2.5]]



[observation]
# The observation space is the same space as the state space
additionalDimensions = 2


[changes]
hasChanges = false
changesPath = 
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The maximum depth to search in the tree, relative to the current belief.
maximumDepth = 1000

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 1000

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchStrategy = ucb(2.0)

estimator = max()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
actionDiscretization = [2, 2]
numInputStepsActions = 2
observationType = continuous

# The maximum L2-distance between observations for them to be considered similar
# Implementation is set to either 10000 or 0. So any value in between is fine
maxObservationDistance = 100 

[simulation]
interactive = true
particlePlotLimit = 100
